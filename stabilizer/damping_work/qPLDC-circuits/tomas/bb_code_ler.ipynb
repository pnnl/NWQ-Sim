{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42b08ec",
   "metadata": {},
   "source": [
    "# Demo. Decoder Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stim\n",
    "import pymatching\n",
    "import sys\n",
    "sys.path.append(\"../QEC-Codes\")  # Adjust the path to import local modules\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QASM export setup (similar to surface_code_ler)\n",
    "import os, sys, importlib\n",
    "qasm_dir = \"./bb_code_qasm\"\n",
    "os.makedirs(qasm_dir, exist_ok=True)\n",
    "# Make sure we can import the conversion utilities\n",
    "sys.path.insert(0, os.path.abspath(\"../../../src\"))\n",
    "import noise_util as ns\n",
    "importlib.reload(ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f3dcb",
   "metadata": {},
   "source": [
    "This demo introduces the basics of decoders, focusing on pymatching and BP+OSD.  \n",
    "\n",
    "**What does decoder do?**  \n",
    "The input is the error information/syndromes/detection events, along with the detector error model that assigns probability/weights to each error mechanism. For each sample, the decoder will return a vector that gives the predictions for the logical errors that have happened (for surface code it's just one value because there is only one logical operator, for general codes it should a vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c2325",
   "metadata": {},
   "source": [
    "### 1. Pymatching for repetition codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c56c1",
   "metadata": {},
   "source": [
    "### Overall Analysis  \n",
    "**LER decreases with increasing code distance and decreasing physical error rate (PER).**  \n",
    "This behavior is consistent with theoretical expectations: as the code distance increases, more physical errors are required to cause a logical error. Similarly, reducing the physical error rate naturally reduces the probability of uncorrectable faults.\n",
    "\n",
    "<!-- (2) **The break-even PER appears to be ≈ 1, meaning the repetition code always outperforms a single bit.**  \n",
    "At first glance, this seems to contradict the classical result that repetition codes only outperform a single bit when \\( p < 0.5 \\) — since majority vote fails above this threshold. However, the classical result is based on **majority vote decoding**, whereas our experiment uses a **matching decoder** (e.g., `pymatching`), which finds the most likely error consistent with the observed syndrome, and can operate effectively even at high error rates. As a result, it can outperform majority vote and maintain logical error suppression for a much wider range of \\( p \\), even approaching 1.\n",
    "\n",
    "(3) **The error threshold appears to be >0.5, meaning that increasing the code distance can suppresses the LER even when the PER is large.**  \n",
    "In standard fault-tolerance theory, a threshold exists below which increasing the code distance improves logical fidelity, and above which larger codes perform worse. Here, because we are using the powerful **matching decoder** on this simple repetition code, the threshold become very large (>0.5). -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afeb2e2",
   "metadata": {},
   "source": [
    "### Analysis of LER Scaling\n",
    "\n",
    "In principle, a QEC code of distance $d$ can correct up to $t = \\lfloor \\frac{d-1}{2} \\rfloor$ errors. In the **ideal** case where all these errors are corrected, the leading-order contribution to the LER should be $p^{t+1}$, where $t + 1 = \\lceil \\frac{d+1}{2} \\rceil$ is the weight of uncorrectable errors. In this case, a log-log plot of LER vs $p$ should produce a straight line with slope $t + 1$.\n",
    "\n",
    "In realistic scenarios, however, decoding is imperfect: some correctable errors of weight ≤ $t$ may still lead to logical failure due to decoder suboptimality. As a result, the effective scaling exponent becomes **less than** $t + 1$.\n",
    "\n",
    "Therefore, by fitting the log-log curve $\\log(\\text{LER})$ vs $ \\log(p) $, we can estimate the effective suppression exponent. The **closer the slope is to t+1**, the more effective the decoder is at approaching the ideal case. Conversely, the difference between the slope and the ideal case quantifies suboptimal correction performance under the given noise model and decoding strategy. We can see that when d increases, the leading degrees become closer to t than t+1 and haves the tendency to drop even below t, indicating the fact that it becomes more and more difficult to correct all the errors up to weight t.  \n",
    "\n",
    "The equation $$slope = (d_{eff}+1)/2 $$can give a metric called the \"effective distance $d_{eff}$\", which should be strictly smaller than the true distance $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715586be",
   "metadata": {},
   "source": [
    "Next, we verify the \"exponential suppression\" of LER in code distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sinter\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97560a5d",
   "metadata": {},
   "source": [
    "This time we try to add the 2Q gate error and measurement error as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dba739",
   "metadata": {},
   "source": [
    "(1) We kind of see a \"threshold behavior\": the LER only suppresses as distance increases when PER is below 10.8\\%. See where the d = 7 and d = 9 line intersect.  \n",
    "(2) We also see a \"break even\" behavior: the LER is belowe PER when p < 10\\%, where encoding into repetition code improves the error resilience compared to a bare qubit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c001ba",
   "metadata": {},
   "source": [
    "### 2. Pymatching for Surface Codes  \n",
    "We directly use Sinter to streamline the sampling. This time we include the BPOSD decoder as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bda64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bposdd import BPOSD\n",
    "import BBcode\n",
    "# from stimbposdd import SinterDecoder_BPOSD, sinter_decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dbeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Compile the C++ code once before the loop\n",
    "print(\"Compiling C++ simulator (MPI)...\")\n",
    "# Adjust the path to your C++ file as needed\n",
    "cpp_file_path = './bb_code_sim.cpp'\n",
    "executable_path = './bb_code_sim'\n",
    "\n",
    "compile_command = [\n",
    "    \"mpicxx\",  # Use MPI compiler wrapper\n",
    "    \"-std=c++17\",\n",
    "    \"-O3\",\n",
    "    \"-I../../../../../\", # Include for NWQ-Sim headers\n",
    "    \"-DMPI_ENABLED\",\n",
    "    \"-o\",\n",
    "    executable_path,\n",
    "    cpp_file_path\n",
    "]\n",
    "try:\n",
    "    subprocess.run(compile_command, check=True, capture_output=True, text=True)\n",
    "    print(\"Compilation successful.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Compilation failed.\")\n",
    "    print(\"--- stdout ---\")\n",
    "    print(e.stdout)\n",
    "    print(\"--- stderr ---\")\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c2d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "circuit = \"bicycle_bivariate_144_12_12_memory_Z\"\n",
    "distance = 12 \n",
    "rounds = 12\n",
    "SHOTS = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "tasks = []\n",
    "T1 = 10 ** -4\n",
    "T2 = 10 ** -4\n",
    "tau = np.array([1.8*10**-6, 2*10**-6, 2.2*10**-6, 2.4*10**-6, 2.6*10**-6, 2.8*10**-6, 3*10**-6])*2s\n",
    "lam = 1/T2 - 1/(2*T1)\n",
    "p_amp = 1 - np.exp(-tau/T1)\n",
    "p_phase = 1 - np.exp(-lam*tau)\n",
    "p_list = (p_amp, p_phase)\n",
    "base_error = 0.000\n",
    "\n",
    "bb_code = BBcode.BBcode(\n",
    "    n=144, k=12, d=12, m=6, l=12,\n",
    "    A=[[3, 0], [0, 1], [0, 2]],\n",
    "    B=[[0, 3], [1, 0], [2, 0]],\n",
    "    shift=[0, 0],\n",
    "    f=[[0, 0], [1, 0], [2, 0], [3, 0], [6, 0], [7, 0], [8, 0], [9, 0], [1, 3], [5, 3], [7, 3], [11, 3]],\n",
    "    g=[[1, 0], [2, 1], [0, 2], [1, 2], [2, 3], [0, 4]],\n",
    "    h=[[0, 0], [0, 1], [1, 1], [0, 2], [0, 3], [1, 3]],\n",
    "    alpha=[[0, 0], [0, 1], [2, 1], [2, 5], [3, 2], [4, 0]],\n",
    "    beta=[[0, 1], [0, 5], [1, 1], [0, 0], [4, 0], [5, 2]],\n",
    ")\n",
    "print(p_amp)\n",
    "print(p_phase)\n",
    "i=0\n",
    "for p_amp, p_phase in zip(p_amp, p_phase):\n",
    "    noise_profile = [0,0,0,0]\n",
    "    circuit = bb_code.build_full_BBcode_circuit(rounds=rounds, noise_profile=noise_profile, observable_type=\"Z\", code_capacity=True)\n",
    "\n",
    "    #Generate stim circuit\n",
    "    model = ns.ErrorModel(circuit)\n",
    "    model.setting_error('Identity', False, f'DEPOLARIZE1({base_error})')\n",
    "    model.setting_error('Single_qubit', False, f'DEPOLARIZE1({base_error})')\n",
    "    model.setting_error('Two_qubit', False, f'DEPOLARIZE2({base_error})')\n",
    "    model.setting_error('Measurement', True, f'PAULI_CHANNEL_1({p_amp/4}, {p_amp/4}, {p_amp/4 + p_phase/2})')\n",
    "    model.setting_error('Reset', True, f'PAULI_CHANNEL_1({p_amp/4}, {p_amp/4}, {p_amp/4 + p_phase/2})')\n",
    "    stim_circuit = model.generate_noisy_circuit()\n",
    "    tasks.append(sinter.Task(circuit=stim_circuit, json_metadata={'d': distance, \"trial\":i}))\n",
    "\n",
    "    #Generate stabsim circuit\n",
    "    model.setting_error('Measurement', True, f'AMPLITUDE_DAMP({p_phase}, {p_amp})')\n",
    "    model.setting_error('Reset', True, f'AMPLITUDE_DAMP({p_phase}, {p_amp})')\n",
    "    stab_circuit = model.generate_noisy_circuit()\n",
    "    qasm_output = ns.stim_to_qasm_with_depolarize_noise(stab_circuit)\n",
    "    # Inject AMPLITUDE_DAMP around M/RESET in QASM using the model settings\n",
    "    qasm_output = ns.inject_amplitude_damp(qasm_output, model)\n",
    "    \n",
    "    try:\n",
    "        qasm_path = os.path.join(qasm_dir, f\"bb_code_d{distance}_p{i}.qasm\")\n",
    "        with open(qasm_path, \"w\") as f:\n",
    "            f.write(qasm_output)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to export QASM for p={p_amp}: {e}\")\n",
    "    i+=1\n",
    "\n",
    "print(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72945c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stim_experiment(tasks, shots):\n",
    "    stim_lers = []\n",
    "    for i,task in enumerate(tasks):\n",
    "        print(f\"trial={task.json_metadata.get('trial')}\")\n",
    "        print(f\"\\n--- Running Stim for p = {i} ---\")\n",
    "        start_time = time.perf_counter()\n",
    "        collected_stats = sinter.collect(\n",
    "            num_workers=1,\n",
    "            tasks=[task],\n",
    "            decoders=['bposd'],\n",
    "            max_shots=shots,\n",
    "            custom_decoders={\n",
    "                'bposd': BPOSD(\n",
    "                    max_iter=1000,\n",
    "                    bp_method=\"ms\",\n",
    "                    osd_order=10,\n",
    "                    osd_method=\"osd_cs\"\n",
    "                )\n",
    "            },\n",
    "            print_progress=False,\n",
    "        )\n",
    "        stim_time = time.perf_counter() - start_time\n",
    "        stat = collected_stats[0]\n",
    "        stim_ler = stat.errors / stat.shots\n",
    "        stim_lers.append(stim_ler)\n",
    "        print(f\"Stim LER: {stim_ler}, Sim+Decoding Time: {stim_time:}s\")\n",
    "    return stim_lers\n",
    "\n",
    "# --- Run Stim-only experiment ---\n",
    "stim_logical_error_rates = run_stim_experiment(tasks, SHOTS)\n",
    "print(\"\\nStim complete.\")\n",
    "print(\"Stim LERs:\", stim_logical_error_rates)\n",
    "\n",
    "try:\n",
    "    results_dir = \"./results\"\n",
    "    p_amp_arr = 1 - np.exp(-tau / T1)\n",
    "    lam = 1 / T2 - 1 / (2 * T1)\n",
    "    p_phase_arr = 1 - np.exp(-lam * tau)\n",
    "    stim_results_path = os.path.join(results_dir, f\"stim_bposd_results_d{distance}_shots{SHOTS}.txt\")\n",
    "    with open(stim_results_path, \"w\") as f:\n",
    "        f.write(\"# tau\\tp_amp\\tp_phase\\tstim_ler\\n\")\n",
    "        for ta, pa, pp, ler in zip(tau, p_amp_arr, p_phase_arr, stim_logical_error_rates):\n",
    "            f.write(f\"{ta}\\t{pa}\\t{pp}\\t{ler}\\n\")\n",
    "    print(f\"Saved Stim (BP+OSD) results to {stim_results_path}\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to save Stim results:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ed161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import bposd_mp\n",
    "\n",
    "def run_cpp_experiment(tasks, shots):\n",
    "    cpp_lers = []\n",
    "    for i, task in enumerate(tasks):\n",
    "        d = task.json_metadata.get('d')\n",
    "        print(f\"\\n--- Running C++ (MPI) for p = {i} ---\")\n",
    "\n",
    "        # Paths\n",
    "        qasm_file_path = os.path.join(qasm_dir, f\"bb_code_d{d}_p{i}.qasm\")\n",
    "        cpp_output_path = os.path.join(qasm_dir, f\"measurements_d{d}_p{i}.txt\")\n",
    "\n",
    "        # MPI execution\n",
    "        num_qubits = task.circuit.num_qubits\n",
    "        mpi_ranks = min(3*os.cpu_count()//4, shots)  # don't spawn more ranks than shots\n",
    "        iters = shots\n",
    "\n",
    "        # env = os.environ.copy()\n",
    "        # env.update({\n",
    "        #     \"OMP_NUM_THREADS\": \"1\",\n",
    "        #     \"OPENBLAS_NUM_THREADS\": \"1\",\n",
    "        #     \"MKL_NUM_THREADS\": \"1\",\n",
    "        #     \"VECLIB_MAXIMUM_THREADS\": \"1\",\n",
    "        #     \"NUMEXPR_NUM_THREADS\": \"1\",\n",
    "        # })\n",
    "\n",
    "        run_command = [\n",
    "            \"mpirun\", \"-np\", str(mpi_ranks),\n",
    "            \"./bb_code_sim\", str(num_qubits), str(iters), qasm_file_path, cpp_output_path\n",
    "        ]\n",
    "\n",
    "        cpp_time = 0\n",
    "        try:\n",
    "            result = subprocess.run(run_command, check=True, capture_output=True, text=True)\n",
    "            for line in result.stdout.strip().split('\\n'):\n",
    "                if \"Total C++ simulation time\" in line:\n",
    "                    try:\n",
    "                        cpp_time = float(line.split(':')[1].strip().replace('s', ''))\n",
    "                    except (ValueError, IndexError):\n",
    "                        pass\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(\"C++ simulation failed to execute.\")\n",
    "            print(\"Return code:\", e.returncode)\n",
    "            print(\"--- stdout ---\")\n",
    "            print(e.stdout)\n",
    "            print(\"--- stderr ---\")\n",
    "            print(e.stderr)\n",
    "            continue\n",
    "\n",
    "        # Post-process + parallel BPOSD decoding\n",
    "        cpp_ler = 0\n",
    "        try:\n",
    "            with open(cpp_output_path, \"r\") as f:\n",
    "                measurement_strings = [line for line in f.read().strip().split('\\n') if line]\n",
    "\n",
    "            if measurement_strings:\n",
    "                measurement_data = np.array(\n",
    "                    [list(map(int, line.split())) for line in measurement_strings],\n",
    "                    dtype=np.int8\n",
    "                ).astype(bool)\n",
    "\n",
    "                m2d_converter = task.circuit.compile_m2d_converter()\n",
    "                cpp_det_samples, cpp_obs_flips = m2d_converter.convert(\n",
    "                    measurements=measurement_data,\n",
    "                    separate_observables=True\n",
    "                )\n",
    "\n",
    "                dem = task.circuit.detector_error_model(\n",
    "                    decompose_errors=True,\n",
    "                    ignore_decomposition_failures=True\n",
    "                )\n",
    "\n",
    "                from beliefmatching import detector_error_model_to_check_matrices\n",
    "                pcm = detector_error_model_to_check_matrices(dem, allow_undecomposed_hyperedges=True)\n",
    "\n",
    "                logicals_matrix = pcm.observables_matrix\n",
    "                decoder_kwargs = dict(\n",
    "                    max_iter=1000,\n",
    "                    bp_method=\"ms\",\n",
    "                    osd_order=10,\n",
    "                    osd_method=\"osd_cs\",\n",
    "                )\n",
    "\n",
    "                total_shots = len(cpp_det_samples)\n",
    "                if total_shots > 0:\n",
    "                    import math\n",
    "                    num_decode_workers = 3*os.cpu_count()//4\n",
    "\n",
    "                    # Simplified chunking: give each worker a single chunk of tasks\n",
    "                    chunk_size = math.ceil(total_shots / num_decode_workers)\n",
    "                    if chunk_size > 0:\n",
    "                        det_chunks = [cpp_det_samples[i:i + chunk_size] for i in range(0, total_shots, chunk_size)]\n",
    "                        obs_chunks = [cpp_obs_flips[i:i + chunk_size] for i in range(0, total_shots, chunk_size)]\n",
    "                    else:\n",
    "                        det_chunks, obs_chunks = [], []\n",
    "\n",
    "\n",
    "                    # Use a process pool to run decoding in parallel, bypassing the GIL.\n",
    "                    # The worker_init function pre-loads each worker process with the large,\n",
    "                    # shared data (check matrix, priors) to avoid costly serialization.\n",
    "                    with ProcessPoolExecutor(\n",
    "                        max_workers=num_decode_workers,\n",
    "                        initializer=bposd_mp.worker_init,\n",
    "                        initargs=(pcm.check_matrix, pcm.priors, logicals_matrix, decoder_kwargs),\n",
    "                    ) as executor:\n",
    "                        # Map the count_errors function over the chunks of data.\n",
    "                        # The sum of errors from all chunks gives the total.\n",
    "                        total_errors = sum(executor.map(bposd_mp.count_errors, det_chunks, obs_chunks))\n",
    "\n",
    "                    cpp_ler = float(total_errors) / float(total_shots)\n",
    "                else:\n",
    "                    cpp_ler = 0.0\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"C++ output file not found at {cpp_output_path}. Skipping analysis.\")\n",
    "\n",
    "        cpp_lers.append(cpp_ler)\n",
    "        print(f\"C++ LER: {cpp_ler}, Sim Time: {cpp_time}s\")\n",
    "    return cpp_lers\n",
    "\n",
    "# --- Run C++-only experiment ---\n",
    "# reuse SHOTS from previous cell\n",
    "cpp_logical_error_rates = run_cpp_experiment(tasks, SHOTS)\n",
    "print(\"\\nC++ complete.\")\n",
    "print(\"C++ LERs:\", cpp_logical_error_rates)\n",
    "# ...existing code...\n",
    "\n",
    "try:\n",
    "    results_dir = \"./results\"\n",
    "    p_amp_arr = 1 - np.exp(-tau / T1)\n",
    "    lam = 1 / T2 - 1 / (2 * T1)\n",
    "    p_phase_arr = 1 - np.exp(-lam * tau)\n",
    "    cpp_results_path = os.path.join(results_dir, f\"cpp_bposd_results_d{distance}_shots{SHOTS}.txt\")\n",
    "    with open(cpp_results_path, \"w\") as f:\n",
    "        f.write(\"# tau\\tp_amp\\tp_phase\\tcpp_ler\\n\")\n",
    "        for ta, pa, pp, ler in zip(tau, p_amp_arr, p_phase_arr, cpp_logical_error_rates):\n",
    "            f.write(f\"{ta}\\t{pa}\\t{pp}\\t{ler}\\n\")\n",
    "    print(f\"Saved C++ (BP+OSD) results to {cpp_results_path}\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to save C++ results:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55292c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, importlib\n",
    "qasm_dir = \"./bb_code_qasm\"\n",
    "os.makedirs(qasm_dir, exist_ok=True)\n",
    "\n",
    "# New: directory for saved results\n",
    "results_dir = \"./results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Make sure we can import the conversion utilities\n",
    "sys.path.insert(0, os.path.abspath(\"../../../src\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16624232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Results ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(tau/T1, stim_logical_error_rates, 'o-', label='Stim LER (BP+OSD)')\n",
    "plt.plot(tau/T1, cpp_logical_error_rates, 's--', label='C++ LER (BP+OSD)')\n",
    "plt.xlabel(\"Physical Error Rate (p)\")\n",
    "plt.ylabel(\"Logical Error Rate (LER)\")\n",
    "plt.title(f\"LER vs. Physical Error Rate for BB Code (d={distance}) Shots= {SHOTS}\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Reload and plot previously saved results ---\n",
    "\n",
    "# Ensure these parameters match the saved files you want to load\n",
    "# SHOTS = 50000000\n",
    "# distance = 12\n",
    "# T1 = 10**-4\n",
    "# T2 = 10**-4\n",
    "\n",
    "try:\n",
    "    results_dir = \"./results\"\n",
    "    stim_results_path = Path(results_dir) / f\"stim_bposd_results_d{distance}_shots{SHOTS}.txt\"\n",
    "    cpp_results_path = Path(results_dir) / f\"cpp_bposd_results_d{distance}_shots{SHOTS}.txt\"\n",
    "\n",
    "    # Check if files exist\n",
    "    if not stim_results_path.exists() or not cpp_results_path.exists():\n",
    "        missing = []\n",
    "        if not stim_results_path.exists(): missing.append(str(stim_results_path))\n",
    "        if not cpp_results_path.exists(): missing.append(str(cpp_results_path))\n",
    "        raise FileNotFoundError(\"Missing results files: \" + \", \".join(missing))\n",
    "\n",
    "    # Function to load data from a tab-separated file\n",
    "    def load_results(path):\n",
    "        data = np.loadtxt(path, comments='#', delimiter='\\t')\n",
    "        # Assuming columns are: tau, p_amp, p_phase, ler\n",
    "        return data[:, 0], data[:, 3]\n",
    "\n",
    "    # Load data\n",
    "    tau_s, stim_ler_s = load_results(stim_results_path)\n",
    "    tau_c, cpp_ler_c = load_results(cpp_results_path)\n",
    "\n",
    "    # --- Plotting ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(tau_s / T1, stim_ler_s, 'o-', label='Stim LER (BP+OSD) [loaded]')\n",
    "    plt.plot(tau_c / T1, cpp_ler_c, 's--', label='C++ LER (BP+OSD) [loaded]')\n",
    "    \n",
    "    plt.xlabel(r\"$\\tau / T1$ Ratio\")\n",
    "    plt.ylabel(\"Logical Error Rate (LER)\")\n",
    "    plt.title(f\"Loaded LER for BB Code (d={distance}), Shots={SHOTS}\")\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to load and plot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
